{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REQUIRED PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning packages\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Reshape\n",
    "\n",
    "# Dataset management packages\n",
    "from spivutils.synthetic_datasets.spid import load_data\n",
    "from spivutils.batch_generators.keras_generator import batch_data\n",
    "from spivutils.common_tools.operations import normalization, vectoraddition, thresholding, imagecropping\n",
    "\n",
    "# \n",
    "from numpy import prod, zeros\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HARDWARE SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detects the avaiable hardware\n",
    "cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Allow memory growth\n",
    "for gpu in gpu_devices:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100#15119\n",
    "batch_size = 5\n",
    "num_epoch  = 10\n",
    "\n",
    "filters = 32\n",
    "units = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "#kernel_size = (1,64,64)\n",
    "#pool_size = (1,3,3)\n",
    "\n",
    "filters = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "kernel_size = [(1,3,3), (1,5,5), (1,7,7), (1,9,9), (1,11,11)]\n",
    "pool_size = [(1,2,2), (1,4,4), (1,6,6), (1,8,8), (1,10,10)]\n",
    "\n",
    "activation_function = 'LeakyReLU'\n",
    "\n",
    "optimizer_function = 'adam'                                      # Weight update after each iteration\n",
    "loss_function      = 'mean_squared_error'                             # Sets the Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports train and validation data\n",
    "(train_x, train_y), (valid_x, valid_y), _ = load_data()\n",
    "\n",
    "# Collect a chunk of the dataset\n",
    "train_x_chunk = train_x[0:int(chunk_size*0.7)]\n",
    "train_y_chunk = train_y[0:int(chunk_size*0.7)]\n",
    "valid_x_chunk = valid_x[0:int(chunk_size*0.15)]\n",
    "valid_y_chunk = valid_y[0:int(chunk_size*0.15)]\n",
    "\n",
    "# Load data in batches to avoid memory overload\n",
    "train_batch = batch_data(train_x_chunk, train_y_chunk, batch_size)\n",
    "valid_batch = batch_data(valid_x_chunk, valid_y_chunk, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standardization(input_data):\n",
    "\n",
    "    output_data = zeros(input_data.shape)\n",
    "\n",
    "    #min = np.min(input_data)\n",
    "    #max = np.max(input_data)\n",
    "\n",
    "    max = 8.75\n",
    "    min = 0\n",
    "\n",
    "    output_data[:, :, :] = (input_data[:, :, :] - min)/(max - min)\n",
    "\n",
    "    #for i in range(2):\n",
    "\n",
    "    #    output_data[:, i, :, :] = (input_data[:, i, :, :] - min)/(max - min)\n",
    "\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Crops a square region with size equals to small image dimension at the center of the image\n",
    "train_batch.add_x_preprocessing_operation(imagecropping)\n",
    "valid_batch.add_x_preprocessing_operation(imagecropping)\n",
    "\n",
    "# Crops a square region with size equals to small image dimension at the center of the image\n",
    "train_batch.add_y_preprocessing_operation(imagecropping)\n",
    "valid_batch.add_y_preprocessing_operation(imagecropping)\n",
    "\n",
    "#\n",
    "train_batch.add_y_preprocessing_operation(vectoraddition)\n",
    "valid_batch.add_y_preprocessing_operation(vectoraddition)\n",
    "\n",
    "# Apply normalization to the input data\n",
    "train_batch.add_x_preprocessing_operation(normalization)\n",
    "valid_batch.add_x_preprocessing_operation(normalization)\n",
    "\n",
    "# Apply normalization to the output data\n",
    "train_batch.add_y_preprocessing_operation(standardization)\n",
    "valid_batch.add_y_preprocessing_operation(standardization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_shape = train_batch[0][0][0,].shape\n",
    "output_shape = train_batch[0][1][0,].shape\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(filters = filters[0], kernel_size = kernel_size[4], activation = activation_function, input_shape = input_shape))\n",
    "model.add(MaxPooling3D(pool_size = pool_size[4]))\n",
    "\n",
    "model.add(Conv3D(filters = filters[1], kernel_size = kernel_size[2], activation = activation_function, input_shape = input_shape))\n",
    "model.add(MaxPooling3D(pool_size = pool_size[2]))\n",
    "\n",
    "model.add(Conv3D(filters = filters[2], kernel_size = kernel_size[1], activation = activation_function, input_shape = input_shape))\n",
    "model.add(MaxPooling3D(pool_size = pool_size[1]))\n",
    "\n",
    "#model.add(Conv3D(filters = filters[3], kernel_size = kernel_size[0], activation = activation_function, input_shape = input_shape))\n",
    "#model.add(MaxPooling3D(pool_size = pool_size[0]))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units = units[0], activation = activation_function))\n",
    "\n",
    "model.add(Dense(units = units[1], activation = activation_function))\n",
    "\n",
    "#model.add(Dense(units = units[2], activation = activation_function))\n",
    "\n",
    "#model.add(Dense(units = units[3], activation = activation_function))\n",
    "\n",
    "#model.add(Dense(units = units[4], activation = activation_function))\n",
    "\n",
    "model.add(Dense(units = units[3], activation = activation_function))\n",
    "\n",
    "model.add(Dense(prod(output_shape)))\n",
    "\n",
    "model.add(Reshape(output_shape))\n",
    "\n",
    "model.compile(loss = loss_function, optimizer = optimizer_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL SUMMARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_batch, validation_data = valid_batch, epochs = num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves model parameters and weights \n",
    "model.save('model_1_23_00.keras')\n",
    "\n",
    "# Saves the model training history\n",
    "#history_df = pd.DataFrame(history.history)\n",
    "#history_df.to_csv('training_history.csv', index = False)\n",
    "\n",
    "with open('model_1_23_00_training_history.json', 'w') as file:\n",
    "    json.dump(history.history, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
