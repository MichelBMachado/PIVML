{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REQUIRED PACKAGGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning packages\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Reshape, Activation, LSTM, TimeDistributed, GlobalMaxPool2D, BatchNormalization, Dropout, InputLayer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as k\n",
    "\n",
    "# Dataset management packages\n",
    "from spivutils.synthetic_datasets.spid import load_data\n",
    "from spivutils.batch_generators.keras_generator import batch_data\n",
    "from spivutils.common_tools.operations import normalization, vectoraddition, thresholding, imagecropping\n",
    "\n",
    "# General purpose packages\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HARDWARE SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detects the avaiable hardware\n",
    "cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Allow memory growth\n",
    "for gpu in gpu_devices:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Layers Hyperparameters\n",
    "cnn_layers = 3\n",
    "filters = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "kernel_size = [(7,7), (5,5), (3,3), (3,3), (3,3)]\n",
    "pool_size = [(6,6), (4,4), (2,2), (2,2), (2,2)]\n",
    "padding = 'same'\n",
    "momentum = 0.9\n",
    "\n",
    "# Dense Layers Hyperparameters\n",
    "dense_layers = 7                                        # Number of Dense Layers\n",
    "dense_units = [2048, 1024, 512, 256, 128, 64, 32]     # Hidden units per layer\n",
    "\n",
    "# LSTM Layers Hyperparameters\n",
    "lstm_layers = 3                                         # Number of LSTM Layers\n",
    "lstm_units = [32, 64, 128, 256, 512, 1024]              # Hidden units per layer\n",
    "\n",
    "# Training Hyperparameters\n",
    "chunk_size = 100 #15119                                        # Slice of the dataset\n",
    "batch_size = 5                                      # Number of patterns shown to the network before weight matrix update\n",
    "epochs = 10                                             # Number of times that the model sees the dataset\n",
    "activation = 'LeakyReLU', 'linear'                           # Sets the activation functions\n",
    "optimizer_function = 'adam'                                      # Weight update after each iteration\n",
    "loss_function = 'mean_squared_error'                             # Sets the Loss function\n",
    "learning_rate = 0.001                                           \n",
    "dropout = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA MANAGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data importing\n",
    "(train_x, train_y), (valid_x, valid_y), _ = load_data()\n",
    "\n",
    "# Collect a chunk of the dataset\n",
    "train_x_chunk = train_x[0:int(chunk_size*0.7)]\n",
    "train_y_chunk = train_y[0:int(chunk_size*0.7)]\n",
    "valid_x_chunk = valid_x[0:int(chunk_size*0.15)]\n",
    "valid_y_chunk = valid_y[0:int(chunk_size*0.15)]\n",
    "\n",
    "# Load data in batches to avoid memory overload\n",
    "train_batch = batch_data(train_x_chunk, train_y_chunk, batch_size)\n",
    "valid_batch = batch_data(valid_x_chunk, valid_y_chunk, batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(input_data):\n",
    "\n",
    "    output_data = np.zeros(input_data.shape)\n",
    "\n",
    "    #min = np.min(input_data)\n",
    "    #max = np.max(input_data)\n",
    "\n",
    "    max = 8.75\n",
    "    min = -8.75\n",
    "\n",
    "    for i in range(2):\n",
    "\n",
    "        output_data[:, i, :, :] = (input_data[:, i, :, :] - min)/(max - min)\n",
    "\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch.add_x_preprocessing_operation(imagecropping)\n",
    "valid_batch.add_x_preprocessing_operation(imagecropping)\n",
    "\n",
    "train_batch.add_y_preprocessing_operation(imagecropping)\n",
    "valid_batch.add_y_preprocessing_operation(imagecropping)\n",
    "\n",
    "train_batch.add_x_preprocessing_operation(normalization)\n",
    "valid_batch.add_x_preprocessing_operation(normalization)\n",
    "\n",
    "train_batch.add_y_preprocessing_operation(standardization)\n",
    "valid_batch.add_y_preprocessing_operation(standardization)\n",
    "\n",
    "train_batch.add_y_preprocessing_operation(vectoraddition)\n",
    "valid_batch.add_y_preprocessing_operation(vectoraddition)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network setup\n",
    "def build_cnn_layers(input_shape, filters, kernel_size, pool_size, activation, padding, momentum, num_layers):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape = input_shape))\n",
    "\n",
    "    for layer in range(0, num_layers):\n",
    "        model.add(Conv2D(filters = filters[layer], kernel_size = kernel_size[layer], padding = padding))\n",
    "        model.add(Activation(activation[0]))\n",
    "        model.add(BatchNormalization(momentum = momentum))\n",
    "        model.add(MaxPooling2D(pool_size = pool_size[layer]))\n",
    "       \n",
    "    model.add(Flatten())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_layers(model, lstm_units, num_layers):\n",
    "\n",
    "    for layer in range(0, num_layers - 1):\n",
    "\n",
    "        model.add(LSTM(units = lstm_units[layer], return_sequences = True))\n",
    "\n",
    "    model.add(LSTM(units = lstm_units[layer], return_sequences = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dense_layers(model, dense_units, activation, num_layers):\n",
    "\n",
    "    for layer in range(0, num_layers):\n",
    "\n",
    "        model.add(Dense(units = dense_units[layer]))\n",
    "        model.add(Activation(activation[0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, output_shape, filters, kernel_size, pool_size, lstm_units, dense_units, activation, padding):\n",
    "\n",
    "    convnet = build_cnn_layers(input_shape[1:], filters, kernel_size, pool_size, activation, padding, momentum, cnn_layers)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(TimeDistributed(convnet, input_shape = input_shape))\n",
    "\n",
    "    build_lstm_layers(model, lstm_units, lstm_layers)\n",
    "\n",
    "    build_dense_layers(model, dense_units, activation, dense_layers)\n",
    "\n",
    "    model.add(Dense(np.prod(output_shape)))\n",
    "    model.add(Activation(activation[1]))\n",
    "\n",
    "    model.add(Reshape(output_shape))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model dimensions\n",
    "input_shape = train_batch[0][0][0,].shape\n",
    "output_shape = train_batch[0][1][0,].shape\n",
    "\n",
    "model = build_model(input_shape, output_shape, filters, kernel_size, pool_size, lstm_units, dense_units, activation, padding)\n",
    "\n",
    "model.compile(loss = loss_function, optimizer = optimizer_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL SUMMARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines an Early Stop callback\n",
    "es_callback = EarlyStopping(monitor = 'val_loss', patience = 3)\n",
    "\n",
    "class ClearMemory(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        gc.collect()\n",
    "        k.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_batch, validation_data = valid_batch, epochs = epochs, callbacks = [ClearMemory(), es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('model_2.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vel_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
